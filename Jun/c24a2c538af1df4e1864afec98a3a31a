When President Cyril Ramaphosa announced the decision to implement an initial 21-day national lockdown in response to the threat posed by the COVID-19 pandemic, he referred to “modelling” on which the decision was based. A media report a few days later based on leaked information claimed that the government had been told that “a slow and inadequate response by government to the outbreak could result in anywhere between 87,900 and 351,000 deaths”. These estimates, the report said, were based on, respectively, population infection rates of 10% to 40%. In late April, the chair of the health minister’s advisory committee sub-committee on public health referred to the early models used by the government as “back-of-the-envelope calculations”, saying they were “flawed and illogical and made wild assumptions”. These assertions have been impossible to fully assess. This is because no official information on the modelling has ever been released – despite its apparently critical role. A briefing by the chair of the health minister’s advisory committee in mid-April sketched some basic details of what the government’s health advisors believed about the likely peak and timing of the epidemic. But no details were given on expected infections, hospital admissions or deaths. A spokesperson for the presidency said that government was withholding such numbers “to avoid panic”. Finally, towards the end of May the health minister hosted an engagement between journalists and some of the modellers government was relying on. It then started releasing details of the models and projections. The predictions of these models for an “optimistic scenario” are that the vast majority of the population will be infected, there will be a peak of 8 million infections in mid-August and in total there will be 40,000 deaths. To understand the significance of these – and the previous numbers – it is useful to consider more broadly what models are and how they are being used in the current context. A theoretical model – whether in epidemiology, economics or even physics – is a simplified representation of how the modeller thinks the world works. To produce estimates or forecasts of how things might play out in the real world, such models need to make assumptions about the strength of relationships between different variables. Those assumptions reflect some combination of the modeller’s beliefs, knowledge and available evidence. To put it differently: modelling is sophisticated guesswork. Where models have been successfully used across different contexts and time periods we can have more confidence in their accuracy and reliability. But models, especially outside sciences like physics, are almost always wrong to some degree. What matters for decision-making is that they are “right enough”. In the current situation, the difference between predicting 35,000 and 40,000 deaths probably won’t change policy decisions, but 5,000 or 500,000 instead of 40,000 might. In the case of South Africa’s COVID-19 response, available information indicates that epidemiological models have played two main roles. First, they have provided predictions of the possible scale of death and illness relative to health system capacity, as well as how this is expected to play out over time. Second, they have been used to assess the success and effects of the government’s intervention strategies. There are reasons to believe that there have been significant failures in both cases, in the modelling itself and especially in the way that it has been used. In recent weeks, the government and its advisors have been keen to emphasise the uncertainty of the modelling predictions. From a methodological point of view, that is the responsible stance. But it’s too little too late. Modelling COVID-19 is challenging in general, but there are at least four additional reasons to be cautious about our COVID-19 models. First, certain key characteristics of SARS-CoV-2 remain unknown and the subject of debate among medical experts. Second, unlike some countries, South Africa does not have detailed data on the dynamics of social interactions and the models presented so far do not use household survey data as a proxy. Nuanced questions therefore aren’t addressed. For example, most cases early on in the epidemic appeared to have been relatively wealthy travellers. But there was no way to model the consequences of domestic workers being exposed by their employers and thereby infecting others in their (poorer) communities. So the structure of South Africa’s models is high level and does not account for country-specific factors. Third, the values for the parameters of the models (representing the strength of relationships between different factors) are being taken from evidence in other countries. They may not actually be the same in South Africa. Finally, the unsystematic nature of aspects of the government’s approach to testing, such as through its community screening programme, makes it much harder to infer the effects of its interventions. There is little reason to believe that government had anything other than good intentions. Nevertheless, the consequences of its lack of sophistication in using evidence and expertise may burden an entire generation of South Africans. A major problem linked to the combination of excessive confidence and secrecy is that the government’s strategy was never clear: although it referred to “flattening the curve” it never stated what its specific objectives were. In the terms of the most influential modelling-based advice during the pandemic, was its strategy “suppression” or “mitigation”? The government and its advisors have made much of the fact that the lockdown probably delayed the peak of the epidemic. But there is no evidence so far that this was worth the cost – since most of the population is expected to be infected anyway. One key claim is that the lockdown bought the country time to prepare the health system. The Imperial model defined the primary objective of “flattening the curve” as reducing ICU admissions below the number of critical care beds. On that dimension, the government’s own modellers predict a peak of 20,000 critical cases in the optimistic scenario and only about 4,000 ICU beds with little increase from the pre-lockdown numbers. By this definition, it has failed dismally. There appears to have been more success with securing supplies of personal protective equipment, quarantine locations, overflow beds and some ventilators. But there is also little evidence that many of those small gains could not have been achieved without such a costly lockdown. Given this, it is concerning that many academics and commentators have praised the success of government’s strategy. This has included the Academy of Sciences, which has asserted  that “strong, science-based governmental leadership has saved many lives, for which South Africa can be thankful”. This is entirely unsubstantiated. First, the full toll of the epidemic will be experienced over time and so it is possible to have fewer deaths at the outset due to a policy intervention being exceeded by a larger number of deaths later because of the limitations of that same policy intervention. Second, the only way to substantiate such claims would be to use models of different scenarios. But we’ve seen that the early models were not credible and the subsequent ones are subject to a great deal of uncertainty. It seems that the government and some of its advisors want to have the best of both worlds: they want to use dramatically incorrect predictions by early models to claim success of their interventions. This is misleading and does not meet the most basic standards by which academics in quantitative disciplines establish causal effects of policy interventions. In an earlier article, I noted that “if the current lockdown fails to drastically curb transmission, which is possible, it would layer one disaster on another … the country may exhaust various resources by the time the potentially more dangerous winter period arrives”. This appears to be the situation in which South Africa finds itself.